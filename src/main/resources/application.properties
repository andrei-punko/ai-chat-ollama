spring.application.name=ai-chat-ollama
#spring.docker.compose.lifecycle-management=start-only
#spring.threads.virtual.enabled=true
server.port=8090

# Use local Ollama
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.options.temperature=0.4

# If running the Ollama Docker Instance separately, then set this property
#spring.docker.compose.enabled=false

# Auto-pulling Models
spring.ai.ollama.init.pull-model-strategy=when_missing
spring.ai.ollama.init.timeout=15m
spring.ai.ollama.init.max-retries=3

# Default Ollama model can be changed by setting the below property.
#spring.ai.ollama.chat.model=llama3.1:8b
spring.ai.ollama.chat.model=gemma3:1b
#spring.ai.ollama.chat.model=gemma3:4b
#spring.ai.ollama.chat.model=qwen2.5-coder:7b
#spring.ai.ollama.chat.model=starcoder2:7b

# If additional Models are required, then set this property
#spring.ai.ollama.init.chat.additional-models=llama3.2, qwen2.5-coder
